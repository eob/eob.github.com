---
layout: post
title: Marvin Minsky on the AI Winter
permalink: /2009/06/marvin-minsky-on-the-ai-winter
thumbnail: aiwinter.png
tags: 
- machine learning
---

I went to the morning half of the Living Heritage AI Workshop held at CSAIL
today and it turned out to be a cool get-together of many of the founders of
artificial intelligence from the MIT AI Lab heydays. Marvin Minsky spoke for a
while, and a small, but interesting, part of his talk was a rundown of why he
  thought AI research essentially came to a halt in the 80s: because focus
  changed from solving small, focused problems to trying to come up with a
  single approach to solve all problems. Here is the rundown he gave of these
  one-size-fits-all approaches, and their shortcomings:

<ul>
<li><b>Neural Networks</b> - tend to get stuck on local peaks</li>
<li><b>Rule-based Systems</b> - don't yet use enough reflective layers</li>
<li><b>Baby Machines</b> - all, so far, have failed to keep growing</li>
<li><b>Statistical Methods</b> - fail to <i>explain</i> what causes exceptions</li>
<li><b>Genetic Programs</b> - fail because they lack explicit goals</li>
<li><b>Situated Action</b> - needs higher-level representations</li>
<li><b>Formal Logic</b> - can't exploit reasoning by analogy</li>
<li><b>Fuzzy Logic</b> - cannot support reflective thinking</li>
<li><b>Simulated Evolution</b> - Fail to learn the causes of failures</li>
<li><b>Algorithmic Probability</b> - More general, but needs approximations.</li>
</ul>

He followed with the comment: "Physicists prosper by showing where old theories
fail. AI-researchers seldom publish their programs' faults."

Another cool tidbit is that he said he never remembered technically admitting
anyone to the AI Lab back in the day. They had a lot of funding at the time,
and people would just show up from universities overseas, and sometimes they
would stay for a week and sometimes they would stick around for good.
